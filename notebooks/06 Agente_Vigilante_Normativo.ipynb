{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7389ee89-8671-4125-b1e9-2a03602a6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7338ae45-3794-49f2-b0bd-15bde3aa9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Parámetros configurables\n",
    "# -------------------------------\n",
    "tipos_deseados = ['circular', 'resolución']  # <- Puedes usar uno o ambos: 'circular', 'resolución'\n",
    "ruta_archivos = \"data_ag1\"  # <- Carpeta donde están los CSV\n",
    "ruta_descargas = \"data_ag1/pdfs_normativas_nuevas\"  # <- Carpeta donde se guardarán los PDFs\n",
    "data_dir = \"data_Ag1\" #<- # Ruta base de archivos CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6097d22-0ef9-454b-85e3-aa80e69a78d4",
   "metadata": {},
   "source": [
    "## 1. Descarga PDFs Normativos Nuevos\n",
    "**Supuesto** : Se asume que el scrapper irá dejando los circulares_nuevas.csv y resoluciones_nuevas.csv de las resoluciones y circulares en carpeta indicada más abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e8027c-ed54-449b-912b-4992e65040f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos a descargar: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando PDFs: 100%|██████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 43.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ya descargado: Circular N_ 42 del 15 de Mayo del 2025.pdf\n",
      "✓ Ya descargado: Circular N_ 38 del 30 de Abril del 2025.pdf\n",
      "✓ Ya descargado: Circular N_ 25 del 03 de Abril del 2025.pdf\n",
      "✓ Ya descargado: Resolución Exenta SII N_ 83 del 03 de Julio del 2025.pdf\n",
      "✓ Ya descargado: Resolución Exenta SII N_ 81 del 30 de Junio del 2025.pdf\n",
      "✓ Ya descargado: Resolución Exenta SII N_ 79 del 26 de Junio del 2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Preparación\n",
    "# -------------------------------\n",
    "os.makedirs(ruta_descargas, exist_ok=True)\n",
    "extensiones = {\n",
    "    'circular': 'circulares_{}.csv',\n",
    "    'resolución': 'resoluciones_{}.csv'\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar archivos según tipo (sin filtrar por año)\n",
    "# -------------------------------\n",
    "df_total = []\n",
    "\n",
    "for tipo in tipos_deseados:\n",
    "    for archivo in os.listdir(ruta_archivos):\n",
    "        if archivo.startswith(extensiones[tipo].split('_')[0]):\n",
    "            nombre_archivo = os.path.join(ruta_archivos, archivo)\n",
    "            if os.path.exists(nombre_archivo):\n",
    "                df = pd.read_csv(nombre_archivo)\n",
    "                df['tipo_documento'] = tipo\n",
    "                df_total.append(df)\n",
    "            else:\n",
    "                print(f\"⚠️ Archivo no encontrado: {nombre_archivo}\")\n",
    "\n",
    "# Unir todo\n",
    "df = pd.concat(df_total, ignore_index=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Descargar PDFs con nombre desde columna 'name'\n",
    "# -------------------------------\n",
    "print(f\"Total de archivos a descargar: {len(df)}\")\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Descargando PDFs\"):\n",
    "    url = row['url']\n",
    "    if pd.isna(url) or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    # Construir nombre de archivo desde 'name', limpiando caracteres no válidos\n",
    "    nombre_limpio = row['name']\n",
    "    nombre_limpio = \"\".join(c if c.isalnum() or c in \" _-()\" else \"_\" for c in nombre_limpio)\n",
    "    nombre_archivo = nombre_limpio.strip() + \".pdf\"\n",
    "    ruta_local = os.path.join(ruta_descargas, nombre_archivo)\n",
    "\n",
    "    if not os.path.exists(ruta_local):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                with open(ruta_local, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "            else:\n",
    "                print(f\"⚠️ Error {r.status_code} al descargar {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Excepción con {url}: {e}\")\n",
    "    else:\n",
    "        print(f\"✓ Ya descargado: {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe355e8-86f1-42a2-8740-9dde418329db",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento Normativas Nuevas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57befdb9-1393-4e71-8f20-de7119c21b39",
   "metadata": {},
   "source": [
    "### 2.1 Combinar Metada Normativas (Circulares + Resoluciones)\n",
    "Combina la metadata de las resoluciones con circulares (si es que hay) en un mismo archivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ebbc8f-9827-43c9-8122-543e114e4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivos combinados y guardados en: data_Ag1\\normativas_nuevas_combinadas.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(data_dir, \"normativas_nuevas_combinadas.csv\")\n",
    "\n",
    "# Cargar archivos CSV de resoluciones y circulares\n",
    "res_files = glob.glob(os.path.join(data_dir, \"resoluciones_*.csv\"))\n",
    "circ_files = glob.glob(os.path.join(data_dir, \"circulares_*.csv\"))\n",
    "\n",
    "def cargar_csv(archivo, tipo):\n",
    "    df = pd.read_csv(archivo)\n",
    "    df['tipo_documento'] = tipo\n",
    "    df['archivo_origen'] = os.path.basename(archivo)\n",
    "    return df\n",
    "\n",
    "# Unir todos los DataFrames\n",
    "dfs = []\n",
    "\n",
    "for archivo in res_files:\n",
    "    dfs.append(cargar_csv(archivo, \"Resolución\"))\n",
    "\n",
    "for archivo in circ_files:\n",
    "    dfs.append(cargar_csv(archivo, \"Circular\"))\n",
    "\n",
    "df_unificado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Guardar el resultado\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"✅ Archivos combinados y guardados en: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583f62d-8d2f-4f29-833b-3bbcfd9a9eba",
   "metadata": {},
   "source": [
    "### 2.2 Extracción Automatizada de Texto desde PDFs con Asociación de Metadatos para Normativas**\n",
    "\n",
    "Extrae contenido textual de normativas legales en PDF y lo vincula con su metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3711bff7-7730-44a0-b9b1-c24c1e0017c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando PDFs: 100%|███████████████████████████████████████████████████████████████████| 7/7 [00:16<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Archivo generado: data_ag1/normativas_nuevas_completas.csv con 7 documentos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- CONFIGURACIÓN --------\n",
    "CSV_PATH = \"data_ag1/normativas_nuevas_combinadas.csv\"\n",
    "PDF_DIR = Path(\"data_ag1/pdfs_normativas_nuevas/\")  # Carpeta donde están los PDFs\n",
    "OUTPUT_CSV = \"data_ag1/normativas_nuevas_completas.csv\"\n",
    "\n",
    "# -------- CARGA DE METADATOS --------\n",
    "df_meta = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Crear índice por nombre limpio (sin extensión)\n",
    "df_meta[\"name_file\"] = df_meta[\"name\"].apply(\n",
    "    lambda x: \"\".join(c if c.isalnum() or c in \" _-()\" else \"_\" for c in str(x)).strip()\n",
    ")\n",
    "df_meta = df_meta.set_index(\"name_file\")\n",
    "\n",
    "# -------- FUNCIÓN PARA EXTRAER TEXTO DE UN PDF --------\n",
    "def extraer_texto_pdf(pdf_path):\n",
    "    try:\n",
    "        texto = extract_text(pdf_path)\n",
    "        return texto.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al procesar {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------- PROCESAMIENTO CON BARRA DE PROGRESO --------\n",
    "registros = []\n",
    "archivos_pdf = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "for archivo in tqdm(archivos_pdf, desc=\"Procesando PDFs\"):\n",
    "    nombre_archivo = archivo.stem  # sin .pdf\n",
    "\n",
    "    texto = extraer_texto_pdf(archivo)\n",
    "    if not texto:\n",
    "        continue\n",
    "\n",
    "    if nombre_archivo in df_meta.index:\n",
    "        fila = df_meta.loc[nombre_archivo]\n",
    "        registros.append({\n",
    "            \"name\": fila[\"name\"],\n",
    "            \"description\": fila[\"description\"],\n",
    "            \"fuente\": fila[\"fuente\"],\n",
    "            \"url\": fila[\"url\"],\n",
    "            \"tipo_documento\": fila.get(\"tipo_documento\", \"\"),  # Incluye tipo_documento si existe\n",
    "            \"cuerpo\": texto\n",
    "        })\n",
    "    else:\n",
    "        registros.append({\n",
    "            \"name\": nombre_archivo,\n",
    "            \"description\": \"\",\n",
    "            \"fuente\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"tipo_documento\": \"\",\n",
    "            \"cuerpo\": texto\n",
    "        })\n",
    "\n",
    "# -------- EXPORTACIÓN A CSV --------\n",
    "df_final = pd.DataFrame(registros)\n",
    "df_final.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n✅ Archivo generado: {OUTPUT_CSV} con {len(df_final)} documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34efae3-b08a-469b-86c8-bff292587e04",
   "metadata": {},
   "source": [
    "### 2.3 Limpieza y reducción de ruido textual en normativas SII\n",
    "Este proceso transforma los documentos legales en texto más claro y enfocado, eliminando encabezados, firmas y secciones repetitivas. Además, aplica reglas básicas de limpieza como eliminación de URLs, símbolos y espacios extra. Se conserva el sentido legal y se mejora la calidad semántica para tareas posteriores como clasificación o embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d3f56d-252a-44e2-90aa-5b57a40f335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 178.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 1496.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo limpio guardado como: data_ag1/normativas_nuevas_limpias.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# -------- Cargar archivo original con PDFs procesados --------\n",
    "df = pd.read_csv(\"data_ag1/normativas_nuevas_completas.csv\")\n",
    "\n",
    "# -------- Asegurar nombres consistentes --------\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# -------- Validación de columnas necesarias --------\n",
    "esperadas = ['cuerpo', 'tipo_documento']\n",
    "for col in esperadas:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Falta columna requerida: '{col}'\")\n",
    "\n",
    "# -------- Función de limpieza avanzada --------\n",
    "def limpieza_avanzada(texto):\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Eliminar encabezados y pies de página comunes\n",
    "    patrones_a_eliminar = [\n",
    "        r'servicio de impuestos internos',\n",
    "        r'santiago, \\d{1,2} de [a-z]+ del \\d{4}',\n",
    "        r'resolución exenta sii n[°º] \\d+.*',\n",
    "        r'circular n[°º] \\d+.*',\n",
    "        r'subdirección de .*',\n",
    "        r'departamento de .*',\n",
    "        r'director(a)? del servicio.*',\n",
    "        r'firmado electrónicamente por.*',\n",
    "        r'este documento ha sido firmado.*',\n",
    "        r'distribúyase.*',\n",
    "        r'este documento es copia fiel.*',\n",
    "        r'esta resolución reemplaza.*',\n",
    "        r'esta circular modifica.*',\n",
    "        r'página \\d+ de \\d+',\n",
    "        r'https?://\\S+',\n",
    "    ]\n",
    "    for patron in patrones_a_eliminar:\n",
    "        texto = re.sub(patron, '', texto)\n",
    "\n",
    "    # Normalizar espacios y eliminar símbolos\n",
    "    texto = re.sub(r'[^\\w\\sáéíóúüñ]', '', texto)\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    return texto\n",
    "\n",
    "# -------- Función para extraer desde parte relevante --------\n",
    "def recortar_a_parte_relevante(texto):\n",
    "    texto = texto.lower()\n",
    "    patrones_inicio = [\n",
    "        r'\\bconsiderando\\b',\n",
    "        r'\\binstrucciones\\b',\n",
    "        r'\\bresuelvo\\b',\n",
    "        r'\\binstrucción general\\b',\n",
    "        r'\\binstrucción específica\\b',\n",
    "    ]\n",
    "    for patron in patrones_inicio:\n",
    "        match = re.search(patron, texto)\n",
    "        if match:\n",
    "            return texto[match.start():]\n",
    "    return texto  # Si no encuentra, deja todo el cuerpo\n",
    "\n",
    "# -------- Aplicar funciones --------\n",
    "df['cuerpo'] = df['cuerpo'].progress_apply(limpieza_avanzada)\n",
    "df['cuerpo'] = df['cuerpo'].progress_apply(recortar_a_parte_relevante)\n",
    "\n",
    "# -------- Guardar el resultado --------\n",
    "output_path = \"data_ag1/normativas_nuevas_limpias.csv\"\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"✅ Archivo limpio guardado como: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd36f8-623d-42af-b097-0d7944815c86",
   "metadata": {},
   "source": [
    "## 3. Clasificador de Agente Vigilante\n",
    "Clasificación de las normativas nuevas en Relevante o No Relevante. Además asigna su respectiva explicación respecto de por qué se asignó tal relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81cc7a2b-0a91-4f87-9f59-74355c16f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.vectorstores import Redis\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e61267b-22dc-4e66-a4b5-2a067fb3cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PARÁMETROS DE CONEXIÓN ----------\n",
    "redis_host = \"127.0.0.1\"\n",
    "redis_port = \"6379\"\n",
    "redis_db = 0\n",
    "redis_password = \"\"\n",
    "redis_username = \"default\"\n",
    "redis_index = \"normativas_sii\"\n",
    "gpt_key = \"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\"  # ← Reemplaza por tu clave real\n",
    "redis_url = f\"redis://{redis_username}:{redis_password}@{redis_host}:{redis_port}/{redis_db}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "525a1238-7783-42c2-815c-1e4230f457ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONTROL DE TOKENS ----------\n",
    "MAX_TOKENS = 30000\n",
    "SAFE_TOKENS = 20000\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
    "\n",
    "def truncar_a_tokens(texto: str, max_tokens: int = SAFE_TOKENS) -> str:\n",
    "    tokens = tokenizer.encode(texto)\n",
    "    return tokenizer.decode(tokens[:max_tokens])\n",
    "\n",
    "def contar_tokens(texto: str) -> int:\n",
    "    return len(tokenizer.encode(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54054107-63cd-4c08-99ca-1289bd627b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PARÁMETROS DE CLASIFICACIÓN ----------\n",
    "INDEX_NAME = redis_index\n",
    "K = 5  # Número de documentos similares desde Redis\n",
    "INPUT_FILE = \"data_ag1/normativas_nuevas_limpias.csv\"\n",
    "OUTPUT_FILE = \"data_ag1/normativas_nuevas_clasificadas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce58a640-1246-45d6-ac4a-9d5d26dceafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CARGA DE NORMATIVAS NUEVAS ----------\n",
    "#df_nuevas = pd.read_csv(INPUT_FILE,nrows=25)\n",
    "df_nuevas = pd.read_csv(INPUT_FILE)\n",
    "df_nuevas = df_nuevas.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "282cc356-f713-469f-9274-316ab6a5ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- EMBEDDINGS Y VECTORSTORE ----------\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    openai_api_key=gpt_key\n",
    ")\n",
    "\n",
    "vectorstore = Redis(\n",
    "    redis_url=redis_url,\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2792f6e2-73fb-4e5d-997a-10c5503608dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Eres un asistente legal experto en normativas tributarias chilenas, especializado en sistemas de cumplimiento normativo automatizado para una fintech.\n",
    "\n",
    "Tu tarea es determinar si una nueva normativa es **Relevante** o **No Relevante** para este sistema, en base al contexto proporcionado.\n",
    "\n",
    "Clasifica como **Relevante** si la normativa se relaciona directamente con alguno de los siguientes temas:  \n",
    "- boleta\n",
    "- comprobante electrónico\n",
    "- registro de compra\n",
    "- registro de venta\n",
    "- cumplimiento tributario\n",
    "- inicio de actividades\n",
    "- medios de pago electrónicos\n",
    "- POS, P.O.S o puntos de venta\n",
    "- operadores y administradores\n",
    "- comercio electrónico\n",
    "- Artículo 68\n",
    "- Artículo 100 bis\n",
    "- psp\n",
    "- proveedores de servicios para procesamiento de pagos\n",
    "- no presencial\n",
    "- pagos electrónicos\n",
    "- Ley 20.393\n",
    "\n",
    "También puedes considerar coincidencias sustantivas con normativas similares ya catalogadas como relevantes en el contexto.\n",
    "\n",
    "**Además**, clasifica como **Relevante** cualquier normativa que **modifique, complemente o haga referencia al Artículo 68 del Código Tributario**, por su impacto directo en los sistemas de cumplimiento tributario automatizado.\n",
    "\n",
    "**No clasifiques como Relevante normativas que solamente:**\n",
    "- regulan convenios o acuerdos institucionales,  \n",
    "- establecen procedimientos internos del SII,  \n",
    "- fijan nóminas, listados o autorizaciones individuales,  \n",
    "a menos que modifiquen directamente procesos generales que afecten el cumplimiento automatizado de la fintech.\n",
    "\n",
    "Si la normativa no trata explícitamente alguno de los temas definidos o no representa un cambio general, clasifica como **No Relevante**.\n",
    "\n",
    "---\n",
    "\n",
    "📄 **Contexto de normativas similares clasificadas previamente:**  \n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "📘 **Nueva normativa:**  \n",
    "{normativa}\n",
    "\n",
    "---\n",
    "\n",
    "Responde estrictamente en el siguiente formato (sin agregar texto adicional):\n",
    "\n",
    "Relevancia: [Relevante|No Relevante]  \n",
    "Explicación: [explicación clara y específica que justifique la decisión según el tema coincidente, o 'No cumple reglas de negocio' si corresponde]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prompt = ChatPromptTemplate(\n",
    " #   input_variables=[\"context\", \"normativa\"],\n",
    "  #  template=template,\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=gpt_key)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbe80611-20c8-4550-b39c-982639f83f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Clasificando normativas: 100%|████████████████████████████████████████████████████████| 7/7 [00:22<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ---------- CLASIFICACIÓN  ----------\n",
    "resultados = []\n",
    "\n",
    "def truncar_a_tokens(texto: str, max_tokens: int) -> str:\n",
    "    tokens = tokenizer.encode(texto)\n",
    "    return texto if len(tokens) <= max_tokens else tokenizer.decode(tokens[:max_tokens])\n",
    "\n",
    "def parsear_respuesta(respuesta: str):\n",
    "    relevancia = \"No Relevante\"\n",
    "    explicacion = \"No se pudo interpretar respuesta\"\n",
    "    for line in respuesta.split(\"\\n\"):\n",
    "        if line.lower().startswith(\"relevancia:\"):\n",
    "            relevancia = line.split(\":\", 1)[1].strip()\n",
    "        elif line.lower().startswith(\"explicación:\"):\n",
    "            explicacion = line.split(\":\", 1)[1].strip()\n",
    "    return relevancia, explicacion\n",
    "\n",
    "for _, row in tqdm(df_nuevas.iterrows(), total=len(df_nuevas), desc=\"🔍 Clasificando normativas\"):\n",
    "    texto_original = str(row[\"cuerpo\"])\n",
    "    metadatos = {\n",
    "        \"name\": row[\"name\"],\n",
    "        \"description\": row[\"description\"],\n",
    "        \"fuente\": row[\"fuente\"],\n",
    "        \"url\": row[\"url\"],\n",
    "        \"tipo_documento\": row.get(\"tipo_documento\", \"\")\n",
    "    }\n",
    "\n",
    "    relevancia = \"No Relevante\"\n",
    "    explicacion = \"Clasificación fallback por error o exceso de tokens.\"\n",
    "\n",
    "    for factor in [1.0, 0.8, 0.6, 0.4, 0.2]:\n",
    "        try:\n",
    "             #texto_truncado = truncar_a_tokens(texto_original, int(SAFE_TOKENS * factor))\n",
    "\n",
    "            from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "            max_tokens = int(SAFE_TOKENS * factor)\n",
    "\n",
    "            # Si cabe completo, usar tal cual\n",
    "            if len(tokenizer.encode(texto_original)) <= max_tokens:\n",
    "                texto_truncado = texto_original\n",
    "            else:\n",
    "                splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "                chunks = splitter.split_text(texto_original)\n",
    "                \n",
    "                texto_truncado = \"\"\n",
    "                tokens_acumulados = 0\n",
    "                for chunk in chunks:\n",
    "                    chunk_tokens = tokenizer.encode(chunk)\n",
    "                    if tokens_acumulados + len(chunk_tokens) > max_tokens:\n",
    "                        break\n",
    "                    texto_truncado += chunk + \"\\n\"\n",
    "                    tokens_acumulados += len(chunk_tokens)\n",
    "\n",
    "            \n",
    "            docs_similares = vectorstore.similarity_search(texto_truncado, k=K)\n",
    "            contexto = \"\\n\\n\".join([doc.page_content for doc in docs_similares])\n",
    "            contexto_truncado = truncar_a_tokens(contexto, int(SAFE_TOKENS * (1.0 - factor)))\n",
    "\n",
    "            normativa = f\"Nombre: {metadatos['name']}\\nDescripción: {metadatos['description']}\\nFuente: {metadatos['fuente']}\\nTexto:\\n{texto_truncado}\"\n",
    "\n",
    "            respuesta = chain.run(context=contexto_truncado, normativa=normativa)\n",
    "            relevancia, explicacion = parsear_respuesta(respuesta)\n",
    "            break  # Éxito, salir del bucle\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Fallo con factor {factor}: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    resultados.append({\n",
    "        \"name\": metadatos[\"name\"],\n",
    "        \"description\": metadatos[\"description\"],\n",
    "        \"fuente\": metadatos[\"fuente\"],\n",
    "        \"url\": metadatos[\"url\"],\n",
    "        \"tipo_documento\": metadatos[\"tipo_documento\"],\n",
    "        \"cuerpo\": texto_original,\n",
    "        \"relevancia\": relevancia,\n",
    "        \"explicacion\": explicacion,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a80f0cfc-010c-41d4-8d30-46b863cf5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clasificación de normativas nuevas completada: data_ag1/normativas_nuevas_clasificadas.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- GUARDAR RESULTADOS ----------\n",
    "df_resultado = pd.DataFrame(resultados)\n",
    "df_resultado.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Clasificación de normativas nuevas completada: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01590be5-df44-457e-b3dd-ec91f4368aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>fuente</th>\n",
       "      <th>url</th>\n",
       "      <th>tipo_documento</th>\n",
       "      <th>cuerpo</th>\n",
       "      <th>relevancia</th>\n",
       "      <th>explicacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Circular N° 25 del 03 de Abril del 2025</td>\n",
       "      <td>Instruye sobre el revalúo de bienes raíces ubi...</td>\n",
       "      <td>Fuente: Subdirección de Avaluaciones</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/circu...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>considerando la modificación del referido artí...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circular N° 38 del 30 de Abril del 2025</td>\n",
       "      <td>Imparte instrucciones sobre la obtención de ro...</td>\n",
       "      <td>Fuente: Subdirección de Asistencia al Contribu...</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/circu...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>considerando estas modificaciones y la necesid...</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>La normativa es relevante porque modifica y co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Circular N° 42 del 15 de Mayo del 2025</td>\n",
       "      <td>Informa tabla de cálculos de reajustes y multa...</td>\n",
       "      <td>Fuente: Subdirección de Fiscalización</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/circu...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>departamento emisor circular nº42 ge00324967 s...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Resolución Exenta SII N° 59 del 06 de Mayo del...</td>\n",
       "      <td>Establece procedimiento para efectuar denuncia...</td>\n",
       "      <td>Fuente: Subdirección Jurídica.</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resolución</td>\n",
       "      <td>considerando 1 que el n1 de la letra a del art...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resolución Exenta SII N° 79 del 26 de Junio de...</td>\n",
       "      <td>Instruye sobre la forma en que los órganos de ...</td>\n",
       "      <td>Fuente: Subdirección de Asistencia al Contribu...</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resolución</td>\n",
       "      <td>considerando 1 que conforme lo dispuesto en lo...</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>La normativa modifica y complementa el Artícul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Resolución Exenta SII N° 81 del 30 de Junio de...</td>\n",
       "      <td>Modifica fecha de entrada en vigencia de la Re...</td>\n",
       "      <td>Fuente: Subdirección de Fiscalización.</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resolución</td>\n",
       "      <td>considerando 1º que la letra a del del artícul...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Resolución Exenta SII N° 83 del 03 de Julio de...</td>\n",
       "      <td>Autoriza como receptor electrónico de document...</td>\n",
       "      <td>Fuente: Subdirección de Asistencia al Contribu...</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resolución</td>\n",
       "      <td>considerando 1 que la resolución exenta n 63 d...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0            Circular N° 25 del 03 de Abril del 2025   \n",
       "1            Circular N° 38 del 30 de Abril del 2025   \n",
       "2             Circular N° 42 del 15 de Mayo del 2025   \n",
       "3  Resolución Exenta SII N° 59 del 06 de Mayo del...   \n",
       "4  Resolución Exenta SII N° 79 del 26 de Junio de...   \n",
       "5  Resolución Exenta SII N° 81 del 30 de Junio de...   \n",
       "6  Resolución Exenta SII N° 83 del 03 de Julio de...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Instruye sobre el revalúo de bienes raíces ubi...   \n",
       "1  Imparte instrucciones sobre la obtención de ro...   \n",
       "2  Informa tabla de cálculos de reajustes y multa...   \n",
       "3  Establece procedimiento para efectuar denuncia...   \n",
       "4  Instruye sobre la forma en que los órganos de ...   \n",
       "5  Modifica fecha de entrada en vigencia de la Re...   \n",
       "6  Autoriza como receptor electrónico de document...   \n",
       "\n",
       "                                              fuente  \\\n",
       "0               Fuente: Subdirección de Avaluaciones   \n",
       "1  Fuente: Subdirección de Asistencia al Contribu...   \n",
       "2              Fuente: Subdirección de Fiscalización   \n",
       "3                     Fuente: Subdirección Jurídica.   \n",
       "4  Fuente: Subdirección de Asistencia al Contribu...   \n",
       "5             Fuente: Subdirección de Fiscalización.   \n",
       "6  Fuente: Subdirección de Asistencia al Contribu...   \n",
       "\n",
       "                                                 url tipo_documento  \\\n",
       "0  https://www.sii.cl/normativa_legislacion/circu...       Circular   \n",
       "1  https://www.sii.cl/normativa_legislacion/circu...       Circular   \n",
       "2  https://www.sii.cl/normativa_legislacion/circu...       Circular   \n",
       "3  https://www.sii.cl/normativa_legislacion/resol...     Resolución   \n",
       "4  https://www.sii.cl/normativa_legislacion/resol...     Resolución   \n",
       "5  https://www.sii.cl/normativa_legislacion/resol...     Resolución   \n",
       "6  https://www.sii.cl/normativa_legislacion/resol...     Resolución   \n",
       "\n",
       "                                              cuerpo    relevancia  \\\n",
       "0  considerando la modificación del referido artí...  No Relevante   \n",
       "1  considerando estas modificaciones y la necesid...     Relevante   \n",
       "2  departamento emisor circular nº42 ge00324967 s...  No Relevante   \n",
       "3  considerando 1 que el n1 de la letra a del art...  No Relevante   \n",
       "4  considerando 1 que conforme lo dispuesto en lo...     Relevante   \n",
       "5  considerando 1º que la letra a del del artícul...  No Relevante   \n",
       "6  considerando 1 que la resolución exenta n 63 d...  No Relevante   \n",
       "\n",
       "                                         explicacion  \n",
       "0  No cumple reglas de negocio. La normativa se c...  \n",
       "1  La normativa es relevante porque modifica y co...  \n",
       "2  No cumple reglas de negocio. La normativa se c...  \n",
       "3  No cumple reglas de negocio. La normativa se c...  \n",
       "4  La normativa modifica y complementa el Artícul...  \n",
       "5  No cumple reglas de negocio. La normativa se c...  \n",
       "6  No cumple reglas de negocio. La normativa se c...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuevas_clasificadas = pd.read_csv(\"data_ag1/normativas_nuevas_clasificadas.csv\")\n",
    "df_nuevas_clasificadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb0ec3-0004-42d9-9a83-b3dee0a880f6",
   "metadata": {},
   "source": [
    "## 4. Indexador de embeddings en Redis\n",
    "Por cada normativa nueva clasificada genera un índice en Redis para ser validada la clasificación con intervención humana. Una vez validada o corregida pasa a ser parte de la KB de normativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5ad24-a8c7-4229-9c80-e9eece0fb809",
   "metadata": {},
   "source": [
    "### 4.1 Prepara documentos\n",
    "Divide documentos en Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78a7d0a4-bff3-4075-9751-23832a7bab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Tokenizador para OpenAI embeddings\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
    "MAX_TOKENS = 30000\n",
    "SAFE_TOKENS = 25000  # margen de seguridad\n",
    "\n",
    "def contar_tokens(texto: str) -> int:\n",
    "    return len(tokenizer.encode(texto))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def resumen_chunks_por_documento(documentos):\n",
    "    conteo = Counter(doc.metadata.get(\"name\", \"Sin nombre\") for doc in documentos)\n",
    "    print(f\"Total de chunks generados: {len(documentos)}\")\n",
    "    print(\"Resumen por documento:\")\n",
    "    for nombre, cantidad in conteo.items():\n",
    "        print(f\"- {nombre}: {cantidad} chunk(s)\")\n",
    "\n",
    "def preparar_documentos(path_csv: str,\n",
    "                        n: int = None,\n",
    "                        anios: Union[int, List[int]] = None,\n",
    "                        chunk_size: int = None,\n",
    "                        chunk_overlap: int = None) -> List[Document]:\n",
    "    \n",
    "    df = pd.read_csv(path_csv).dropna(subset=[\"cuerpo\"])\n",
    "\n",
    "    # Filtro por año\n",
    "    if anios:\n",
    "        if isinstance(anios, int):\n",
    "            anios = [anios]\n",
    "        df = df[df[\"name\"].astype(str).str.contains(\"|\".join([str(anio) for anio in anios]))]\n",
    "\n",
    "    if n:\n",
    "        df = df.head(n)\n",
    "\n",
    "    documentos = []\n",
    "    splitter = None\n",
    "\n",
    "    if chunk_size and chunk_overlap:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        metadatos = {\n",
    "            \"name\": row.get(\"name\", \"\"),\n",
    "            \"description\": row.get(\"description\", \"\"),\n",
    "            \"fuente\": row.get(\"fuente\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"tipo_documento\": row.get(\"tipo_documento\", \"\"),\n",
    "            \"relevancia\": row.get(\"relevancia\", \"\"),\n",
    "            \"explicacion\": row.get(\"explicacion\", \"\")\n",
    "        }\n",
    "\n",
    "        cuerpo = str(row[\"cuerpo\"]).strip()\n",
    "        total_tokens = contar_tokens(cuerpo)\n",
    "\n",
    "        if total_tokens <= SAFE_TOKENS:\n",
    "            documentos.append(Document(page_content=cuerpo, metadata=metadatos))\n",
    "        elif splitter:\n",
    "            partes = splitter.split_text(cuerpo)\n",
    "            for parte in partes:\n",
    "                if contar_tokens(parte) <= SAFE_TOKENS:\n",
    "                    documentos.append(Document(page_content=parte, metadata=metadatos))\n",
    "                else:\n",
    "                    print(f\"⚠️ Chunk excede el máximo seguro de tokens, se omitió parcialmente.\")\n",
    "        else:\n",
    "            print(f\"⚠️ Documento omitido por superar límite de tokens y no hay splitter definido.\")\n",
    "\n",
    "    return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "089f5158-dd27-4ada-80c6-b68d891ea5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar muestra de documentos cargados\n",
    "documentos = preparar_documentos(\n",
    "    path_csv=\"data_ag1/normativas_nuevas_clasificadas.csv\",\n",
    "    n=None,\n",
    "    anios= None,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0ed1196-718d-479d-b085-7bbf7e2b386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks generados: 7\n",
      "Resumen por documento:\n",
      "- Circular N° 25 del 03 de Abril del 2025: 1 chunk(s)\n",
      "- Circular N° 38 del 30 de Abril del 2025: 1 chunk(s)\n",
      "- Circular N° 42 del 15 de Mayo del 2025: 1 chunk(s)\n",
      "- Resolución Exenta SII N° 59 del 06 de Mayo del 2025: 1 chunk(s)\n",
      "- Resolución Exenta SII N° 79 del 26 de Junio del 2025: 1 chunk(s)\n",
      "- Resolución Exenta SII N° 81 del 30 de Junio del 2025: 1 chunk(s)\n",
      "- Resolución Exenta SII N° 83 del 03 de Julio del 2025: 1 chunk(s)\n"
     ]
    }
   ],
   "source": [
    "resumen_chunks_por_documento(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1caeca-e79f-4dd8-890c-e97cb78baebc",
   "metadata": {},
   "source": [
    "### 4.2 Indexación en Redis\n",
    "Registra normativas y sus embeddings en Redis. Cuando una normativa nueva es indexada quedará con el campo \"validado = 0\" , que significa que aún no has sido validada por un humano. Mediante alguna GUI, el usuario podrá indicar que ya está validada o corregida su relevancia y explicación, pasando así a un valor \"validado=1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d74b42a-d2f3-4fde-8250-704e04ca361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexar_en_redis(documentos, redis_url, redis_index, gpt_key, eliminar_anteriores=False):\n",
    "    from langchain_community.vectorstores.redis import Redis\n",
    "    from langchain_community.embeddings import OpenAIEmbeddings\n",
    "    from langchain.schema import Document\n",
    "    import redis as redis_client\n",
    "    import traceback\n",
    "    import pandas as pd\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",  # Dimensión fija de 6144\n",
    "    openai_api_key=gpt_key\n",
    ")\n",
    "\n",
    "    if eliminar_anteriores:\n",
    "        try:\n",
    "            r = redis_client.from_url(redis_url)\n",
    "            r.ft(redis_index).dropindex(delete_documents=True)\n",
    "            print(f\"🧹 Índice '{redis_index}' eliminado correctamente.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ No se pudo eliminar el índice '{redis_index}' (puede que no exista): {e}\")\n",
    "\n",
    "    index_schema = {\n",
    "        \"name\": \"TEXT\",\n",
    "        \"description\": \"TEXT\",\n",
    "        \"fuente\": \"TEXT\",\n",
    "        \"url\": \"TEXT\",\n",
    "        \"tipo_documento\": \"TEXT\",\n",
    "        \"relevancia\": \"TEXT\",\n",
    "        \"explicacion\": \"TEXT\",\n",
    "        \"validado\": \"NUMERIC\"\n",
    "    }\n",
    "\n",
    "    print(f\"🚀 Iniciando indexación de {len(documentos)} documentos (con control de tokens)...\")\n",
    "\n",
    "    errores = []\n",
    "    indexados = []\n",
    "    lote_actual = []\n",
    "    tokens_lote = 0\n",
    "\n",
    "    for i, doc in enumerate(documentos):\n",
    "        texto = doc.page_content\n",
    "        tokens_doc = contar_tokens(texto)\n",
    "\n",
    "        if tokens_doc > SAFE_TOKENS:\n",
    "            errores.append({\n",
    "                \"indice\": i + 1,\n",
    "                \"name\": doc.metadata.get('name', f'doc_{i}'),\n",
    "                \"tokens\": tokens_doc,\n",
    "                \"error\": \"Supera SAFE_TOKENS individualmente\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 👇 Añadir campo \"validado\"\n",
    "        doc.metadata[\"validado\"] = 0\n",
    "        \n",
    "        if tokens_lote + tokens_doc > SAFE_TOKENS:\n",
    "            # Indexar lote actual\n",
    "            try:\n",
    "                Redis.from_documents(\n",
    "                    lote_actual,\n",
    "                    embedding=embeddings,\n",
    "                    redis_url=redis_url,\n",
    "                    index_name=redis_index,\n",
    "                    index_schema=index_schema\n",
    "                )\n",
    "                indexados.extend(lote_actual)\n",
    "                print(f\"✅ Lote de {len(lote_actual)} documentos indexado.\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error al indexar lote: {str(e)}\")\n",
    "                for j, d in enumerate(lote_actual):\n",
    "                    errores.append({\n",
    "                        \"indice\": i - len(lote_actual) + j + 1,\n",
    "                        \"name\": d.metadata.get('name', f'doc_{j}'),\n",
    "                        \"tokens\": contar_tokens(d.page_content),\n",
    "                        \"error\": f\"Error en lote: {str(e)}\"\n",
    "                    })\n",
    "            # Resetear lote\n",
    "            lote_actual = []\n",
    "            tokens_lote = 0\n",
    "\n",
    "        lote_actual.append(doc)\n",
    "        tokens_lote += tokens_doc\n",
    "\n",
    "    # Indexar último lote si quedó algo pendiente\n",
    "    if lote_actual:\n",
    "        try:\n",
    "            Redis.from_documents(\n",
    "                lote_actual,\n",
    "                embedding=embeddings,\n",
    "                redis_url=redis_url,\n",
    "                index_name=redis_index,\n",
    "                index_schema=index_schema\n",
    "            )\n",
    "            indexados.extend(lote_actual)\n",
    "            print(f\"✅ Último lote de {len(lote_actual)} documentos indexado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en último lote: {str(e)}\")\n",
    "            for j, d in enumerate(lote_actual):\n",
    "                errores.append({\n",
    "                    \"indice\": len(indexados) + j + 1,\n",
    "                    \"name\": d.metadata.get('name', f'doc_{j}'),\n",
    "                    \"tokens\": contar_tokens(d.page_content),\n",
    "                    \"error\": f\"Error en lote final: {str(e)}\"\n",
    "                })\n",
    "\n",
    "    print(f\"📦 Total indexados: {len(indexados)} / {len(documentos)}\")\n",
    "\n",
    "    if errores:\n",
    "        df_err = pd.DataFrame(errores)\n",
    "        df_err.to_csv(\"errores_indexacion.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(\"📄 Errores registrados en 'errores_indexacion.csv'\")\n",
    "\n",
    "    return len(indexados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a625972b-b113-49f8-b56b-326f141131c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`index_schema` does not match generated metadata schema.\n",
      "If you meant to manually override the schema, please ignore this message.\n",
      "index_schema: {'name': 'TEXT', 'description': 'TEXT', 'fuente': 'TEXT', 'url': 'TEXT', 'tipo_documento': 'TEXT', 'relevancia': 'TEXT', 'explicacion': 'TEXT', 'validado': 'NUMERIC'}\n",
      "generated_schema: {'text': [{'name': 'name'}, {'name': 'description'}, {'name': 'fuente'}, {'name': 'url'}, {'name': 'tipo_documento'}, {'name': 'relevancia'}, {'name': 'explicacion'}], 'numeric': [{'name': 'validado'}], 'tag': []}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando indexación de 7 documentos (con control de tokens)...\n",
      "✅ Último lote de 7 documentos indexado.\n",
      "📦 Total indexados: 7 / 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexar_en_redis(documentos, redis_url, redis_index, gpt_key, eliminar_anteriores=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
