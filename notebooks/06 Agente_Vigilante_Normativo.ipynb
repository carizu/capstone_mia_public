{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7389ee89-8671-4125-b1e9-2a03602a6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7338ae45-3794-49f2-b0bd-15bde3aa9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Par√°metros configurables\n",
    "# -------------------------------\n",
    "tipos_deseados = ['circular', 'resoluci√≥n']  # <- Puedes usar uno o ambos: 'circular', 'resoluci√≥n'\n",
    "ruta_archivos = \"data_ag1\"  # <- Carpeta donde est√°n los CSV\n",
    "ruta_descargas = \"data_ag1/pdfs_normativas_nuevas\"  # <- Carpeta donde se guardar√°n los PDFs\n",
    "data_dir = \"data_Ag1\" #<- # Ruta base de archivos CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6097d22-0ef9-454b-85e3-aa80e69a78d4",
   "metadata": {},
   "source": [
    "## 1. Descarga PDFs Normativos Nuevos\n",
    "**Supuesto** : Se asume que el scrapper ir√° dejando los circulares_nuevas.csv y resoluciones_nuevas.csv de las resoluciones y circulares en carpeta indicada m√°s abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e8027c-ed54-449b-912b-4992e65040f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos a descargar: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 43.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ya descargado: Circular N_ 42 del 15 de Mayo del 2025.pdf\n",
      "‚úì Ya descargado: Circular N_ 38 del 30 de Abril del 2025.pdf\n",
      "‚úì Ya descargado: Circular N_ 25 del 03 de Abril del 2025.pdf\n",
      "‚úì Ya descargado: Resoluci√≥n Exenta SII N_ 83 del 03 de Julio del 2025.pdf\n",
      "‚úì Ya descargado: Resoluci√≥n Exenta SII N_ 81 del 30 de Junio del 2025.pdf\n",
      "‚úì Ya descargado: Resoluci√≥n Exenta SII N_ 79 del 26 de Junio del 2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Preparaci√≥n\n",
    "# -------------------------------\n",
    "os.makedirs(ruta_descargas, exist_ok=True)\n",
    "extensiones = {\n",
    "    'circular': 'circulares_{}.csv',\n",
    "    'resoluci√≥n': 'resoluciones_{}.csv'\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar archivos seg√∫n tipo (sin filtrar por a√±o)\n",
    "# -------------------------------\n",
    "df_total = []\n",
    "\n",
    "for tipo in tipos_deseados:\n",
    "    for archivo in os.listdir(ruta_archivos):\n",
    "        if archivo.startswith(extensiones[tipo].split('_')[0]):\n",
    "            nombre_archivo = os.path.join(ruta_archivos, archivo)\n",
    "            if os.path.exists(nombre_archivo):\n",
    "                df = pd.read_csv(nombre_archivo)\n",
    "                df['tipo_documento'] = tipo\n",
    "                df_total.append(df)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Archivo no encontrado: {nombre_archivo}\")\n",
    "\n",
    "# Unir todo\n",
    "df = pd.concat(df_total, ignore_index=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Descargar PDFs con nombre desde columna 'name'\n",
    "# -------------------------------\n",
    "print(f\"Total de archivos a descargar: {len(df)}\")\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Descargando PDFs\"):\n",
    "    url = row['url']\n",
    "    if pd.isna(url) or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    # Construir nombre de archivo desde 'name', limpiando caracteres no v√°lidos\n",
    "    nombre_limpio = row['name']\n",
    "    nombre_limpio = \"\".join(c if c.isalnum() or c in \" _-()\" else \"_\" for c in nombre_limpio)\n",
    "    nombre_archivo = nombre_limpio.strip() + \".pdf\"\n",
    "    ruta_local = os.path.join(ruta_descargas, nombre_archivo)\n",
    "\n",
    "    if not os.path.exists(ruta_local):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                with open(ruta_local, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Error {r.status_code} al descargar {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Excepci√≥n con {url}: {e}\")\n",
    "    else:\n",
    "        print(f\"‚úì Ya descargado: {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe355e8-86f1-42a2-8740-9dde418329db",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento Normativas Nuevas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57befdb9-1393-4e71-8f20-de7119c21b39",
   "metadata": {},
   "source": [
    "### 2.1 Combinar Metada Normativas (Circulares + Resoluciones)\n",
    "Combina la metadata de las resoluciones con circulares (si es que hay) en un mismo archivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ebbc8f-9827-43c9-8122-543e114e4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivos combinados y guardados en: data_Ag1\\normativas_nuevas_combinadas.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(data_dir, \"normativas_nuevas_combinadas.csv\")\n",
    "\n",
    "# Cargar archivos CSV de resoluciones y circulares\n",
    "res_files = glob.glob(os.path.join(data_dir, \"resoluciones_*.csv\"))\n",
    "circ_files = glob.glob(os.path.join(data_dir, \"circulares_*.csv\"))\n",
    "\n",
    "def cargar_csv(archivo, tipo):\n",
    "    df = pd.read_csv(archivo)\n",
    "    df['tipo_documento'] = tipo\n",
    "    df['archivo_origen'] = os.path.basename(archivo)\n",
    "    return df\n",
    "\n",
    "# Unir todos los DataFrames\n",
    "dfs = []\n",
    "\n",
    "for archivo in res_files:\n",
    "    dfs.append(cargar_csv(archivo, \"Resoluci√≥n\"))\n",
    "\n",
    "for archivo in circ_files:\n",
    "    dfs.append(cargar_csv(archivo, \"Circular\"))\n",
    "\n",
    "df_unificado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Guardar el resultado\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"‚úÖ Archivos combinados y guardados en: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583f62d-8d2f-4f29-833b-3bbcfd9a9eba",
   "metadata": {},
   "source": [
    "### 2.2 Extracci√≥n Automatizada de Texto desde PDFs con Asociaci√≥n de Metadatos para Normativas**\n",
    "\n",
    "Extrae contenido textual de normativas legales en PDF y lo vincula con su metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3711bff7-7730-44a0-b9b1-c24c1e0017c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:16<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Archivo generado: data_ag1/normativas_nuevas_completas.csv con 7 documentos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- CONFIGURACI√ìN --------\n",
    "CSV_PATH = \"data_ag1/normativas_nuevas_combinadas.csv\"\n",
    "PDF_DIR = Path(\"data_ag1/pdfs_normativas_nuevas/\")  # Carpeta donde est√°n los PDFs\n",
    "OUTPUT_CSV = \"data_ag1/normativas_nuevas_completas.csv\"\n",
    "\n",
    "# -------- CARGA DE METADATOS --------\n",
    "df_meta = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Crear √≠ndice por nombre limpio (sin extensi√≥n)\n",
    "df_meta[\"name_file\"] = df_meta[\"name\"].apply(\n",
    "    lambda x: \"\".join(c if c.isalnum() or c in \" _-()\" else \"_\" for c in str(x)).strip()\n",
    ")\n",
    "df_meta = df_meta.set_index(\"name_file\")\n",
    "\n",
    "# -------- FUNCI√ìN PARA EXTRAER TEXTO DE UN PDF --------\n",
    "def extraer_texto_pdf(pdf_path):\n",
    "    try:\n",
    "        texto = extract_text(pdf_path)\n",
    "        return texto.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al procesar {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------- PROCESAMIENTO CON BARRA DE PROGRESO --------\n",
    "registros = []\n",
    "archivos_pdf = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "for archivo in tqdm(archivos_pdf, desc=\"Procesando PDFs\"):\n",
    "    nombre_archivo = archivo.stem  # sin .pdf\n",
    "\n",
    "    texto = extraer_texto_pdf(archivo)\n",
    "    if not texto:\n",
    "        continue\n",
    "\n",
    "    if nombre_archivo in df_meta.index:\n",
    "        fila = df_meta.loc[nombre_archivo]\n",
    "        registros.append({\n",
    "            \"name\": fila[\"name\"],\n",
    "            \"description\": fila[\"description\"],\n",
    "            \"fuente\": fila[\"fuente\"],\n",
    "            \"url\": fila[\"url\"],\n",
    "            \"tipo_documento\": fila.get(\"tipo_documento\", \"\"),  # Incluye tipo_documento si existe\n",
    "            \"cuerpo\": texto\n",
    "        })\n",
    "    else:\n",
    "        registros.append({\n",
    "            \"name\": nombre_archivo,\n",
    "            \"description\": \"\",\n",
    "            \"fuente\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"tipo_documento\": \"\",\n",
    "            \"cuerpo\": texto\n",
    "        })\n",
    "\n",
    "# -------- EXPORTACI√ìN A CSV --------\n",
    "df_final = pd.DataFrame(registros)\n",
    "df_final.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n‚úÖ Archivo generado: {OUTPUT_CSV} con {len(df_final)} documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34efae3-b08a-469b-86c8-bff292587e04",
   "metadata": {},
   "source": [
    "### 2.3 Limpieza y reducci√≥n de ruido textual en normativas SII\n",
    "Este proceso transforma los documentos legales en texto m√°s claro y enfocado, eliminando encabezados, firmas y secciones repetitivas. Adem√°s, aplica reglas b√°sicas de limpieza como eliminaci√≥n de URLs, s√≠mbolos y espacios extra. Se conserva el sentido legal y se mejora la calidad sem√°ntica para tareas posteriores como clasificaci√≥n o embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d3f56d-252a-44e2-90aa-5b57a40f335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 178.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 1496.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo limpio guardado como: data_ag1/normativas_nuevas_limpias.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# -------- Cargar archivo original con PDFs procesados --------\n",
    "df = pd.read_csv(\"data_ag1/normativas_nuevas_completas.csv\")\n",
    "\n",
    "# -------- Asegurar nombres consistentes --------\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# -------- Validaci√≥n de columnas necesarias --------\n",
    "esperadas = ['cuerpo', 'tipo_documento']\n",
    "for col in esperadas:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Falta columna requerida: '{col}'\")\n",
    "\n",
    "# -------- Funci√≥n de limpieza avanzada --------\n",
    "def limpieza_avanzada(texto):\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Eliminar encabezados y pies de p√°gina comunes\n",
    "    patrones_a_eliminar = [\n",
    "        r'servicio de impuestos internos',\n",
    "        r'santiago, \\d{1,2} de [a-z]+ del \\d{4}',\n",
    "        r'resoluci√≥n exenta sii n[¬∞¬∫] \\d+.*',\n",
    "        r'circular n[¬∞¬∫] \\d+.*',\n",
    "        r'subdirecci√≥n de .*',\n",
    "        r'departamento de .*',\n",
    "        r'director(a)? del servicio.*',\n",
    "        r'firmado electr√≥nicamente por.*',\n",
    "        r'este documento ha sido firmado.*',\n",
    "        r'distrib√∫yase.*',\n",
    "        r'este documento es copia fiel.*',\n",
    "        r'esta resoluci√≥n reemplaza.*',\n",
    "        r'esta circular modifica.*',\n",
    "        r'p√°gina \\d+ de \\d+',\n",
    "        r'https?://\\S+',\n",
    "    ]\n",
    "    for patron in patrones_a_eliminar:\n",
    "        texto = re.sub(patron, '', texto)\n",
    "\n",
    "    # Normalizar espacios y eliminar s√≠mbolos\n",
    "    texto = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√º√±]', '', texto)\n",
    "    texto = re.sub(r'\\n+', '\\n', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    return texto\n",
    "\n",
    "# -------- Funci√≥n para extraer desde parte relevante --------\n",
    "def recortar_a_parte_relevante(texto):\n",
    "    texto = texto.lower()\n",
    "    patrones_inicio = [\n",
    "        r'\\bconsiderando\\b',\n",
    "        r'\\binstrucciones\\b',\n",
    "        r'\\bresuelvo\\b',\n",
    "        r'\\binstrucci√≥n general\\b',\n",
    "        r'\\binstrucci√≥n espec√≠fica\\b',\n",
    "    ]\n",
    "    for patron in patrones_inicio:\n",
    "        match = re.search(patron, texto)\n",
    "        if match:\n",
    "            return texto[match.start():]\n",
    "    return texto  # Si no encuentra, deja todo el cuerpo\n",
    "\n",
    "# -------- Aplicar funciones --------\n",
    "df['cuerpo'] = df['cuerpo'].progress_apply(limpieza_avanzada)\n",
    "df['cuerpo'] = df['cuerpo'].progress_apply(recortar_a_parte_relevante)\n",
    "\n",
    "# -------- Guardar el resultado --------\n",
    "output_path = \"data_ag1/normativas_nuevas_limpias.csv\"\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"‚úÖ Archivo limpio guardado como: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd36f8-623d-42af-b097-0d7944815c86",
   "metadata": {},
   "source": [
    "## 3. Clasificador de Agente Vigilante\n",
    "Clasificaci√≥n de las normativas nuevas en Relevante o No Relevante. Adem√°s asigna su respectiva explicaci√≥n respecto de por qu√© se asign√≥ tal relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81cc7a2b-0a91-4f87-9f59-74355c16f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.vectorstores import Redis\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e61267b-22dc-4e66-a4b5-2a067fb3cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PAR√ÅMETROS DE CONEXI√ìN ----------\n",
    "redis_host = \"127.0.0.1\"\n",
    "redis_port = \"6379\"\n",
    "redis_db = 0\n",
    "redis_password = \"\"\n",
    "redis_username = \"default\"\n",
    "redis_index = \"normativas_sii\"\n",
    "gpt_key = \"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\"  # ‚Üê Reemplaza por tu clave real\n",
    "redis_url = f\"redis://{redis_username}:{redis_password}@{redis_host}:{redis_port}/{redis_db}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "525a1238-7783-42c2-815c-1e4230f457ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONTROL DE TOKENS ----------\n",
    "MAX_TOKENS = 30000\n",
    "SAFE_TOKENS = 20000\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
    "\n",
    "def truncar_a_tokens(texto: str, max_tokens: int = SAFE_TOKENS) -> str:\n",
    "    tokens = tokenizer.encode(texto)\n",
    "    return tokenizer.decode(tokens[:max_tokens])\n",
    "\n",
    "def contar_tokens(texto: str) -> int:\n",
    "    return len(tokenizer.encode(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54054107-63cd-4c08-99ca-1289bd627b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PAR√ÅMETROS DE CLASIFICACI√ìN ----------\n",
    "INDEX_NAME = redis_index\n",
    "K = 5  # N√∫mero de documentos similares desde Redis\n",
    "INPUT_FILE = \"data_ag1/normativas_nuevas_limpias.csv\"\n",
    "OUTPUT_FILE = \"data_ag1/normativas_nuevas_clasificadas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce58a640-1246-45d6-ac4a-9d5d26dceafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CARGA DE NORMATIVAS NUEVAS ----------\n",
    "#df_nuevas = pd.read_csv(INPUT_FILE,nrows=25)\n",
    "df_nuevas = pd.read_csv(INPUT_FILE)\n",
    "df_nuevas = df_nuevas.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "282cc356-f713-469f-9274-316ab6a5ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- EMBEDDINGS Y VECTORSTORE ----------\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    openai_api_key=gpt_key\n",
    ")\n",
    "\n",
    "vectorstore = Redis(\n",
    "    redis_url=redis_url,\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2792f6e2-73fb-4e5d-997a-10c5503608dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Eres un asistente legal experto en normativas tributarias chilenas, especializado en sistemas de cumplimiento normativo automatizado para una fintech.\n",
    "\n",
    "Tu tarea es determinar si una nueva normativa es **Relevante** o **No Relevante** para este sistema, en base al contexto proporcionado.\n",
    "\n",
    "Clasifica como **Relevante** si la normativa se relaciona directamente con alguno de los siguientes temas:  \n",
    "- boleta\n",
    "- comprobante electr√≥nico\n",
    "- registro de compra\n",
    "- registro de venta\n",
    "- cumplimiento tributario\n",
    "- inicio de actividades\n",
    "- medios de pago electr√≥nicos\n",
    "- POS, P.O.S o puntos de venta\n",
    "- operadores y administradores\n",
    "- comercio electr√≥nico\n",
    "- Art√≠culo 68\n",
    "- Art√≠culo 100 bis\n",
    "- psp\n",
    "- proveedores de servicios para procesamiento de pagos\n",
    "- no presencial\n",
    "- pagos electr√≥nicos\n",
    "- Ley 20.393\n",
    "\n",
    "Tambi√©n puedes considerar coincidencias sustantivas con normativas similares ya catalogadas como relevantes en el contexto.\n",
    "\n",
    "**Adem√°s**, clasifica como **Relevante** cualquier normativa que **modifique, complemente o haga referencia al Art√≠culo 68 del C√≥digo Tributario**, por su impacto directo en los sistemas de cumplimiento tributario automatizado.\n",
    "\n",
    "**No clasifiques como Relevante normativas que solamente:**\n",
    "- regulan convenios o acuerdos institucionales,  \n",
    "- establecen procedimientos internos del SII,  \n",
    "- fijan n√≥minas, listados o autorizaciones individuales,  \n",
    "a menos que modifiquen directamente procesos generales que afecten el cumplimiento automatizado de la fintech.\n",
    "\n",
    "Si la normativa no trata expl√≠citamente alguno de los temas definidos o no representa un cambio general, clasifica como **No Relevante**.\n",
    "\n",
    "---\n",
    "\n",
    "üìÑ **Contexto de normativas similares clasificadas previamente:**  \n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "üìò **Nueva normativa:**  \n",
    "{normativa}\n",
    "\n",
    "---\n",
    "\n",
    "Responde estrictamente en el siguiente formato (sin agregar texto adicional):\n",
    "\n",
    "Relevancia: [Relevante|No Relevante]  \n",
    "Explicaci√≥n: [explicaci√≥n clara y espec√≠fica que justifique la decisi√≥n seg√∫n el tema coincidente, o 'No cumple reglas de negocio' si corresponde]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prompt = ChatPromptTemplate(\n",
    " #   input_variables=[\"context\", \"normativa\"],\n",
    "  #  template=template,\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=gpt_key)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbe80611-20c8-4550-b39c-982639f83f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Clasificando normativas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ---------- CLASIFICACI√ìN  ----------\n",
    "resultados = []\n",
    "\n",
    "def truncar_a_tokens(texto: str, max_tokens: int) -> str:\n",
    "    tokens = tokenizer.encode(texto)\n",
    "    return texto if len(tokens) <= max_tokens else tokenizer.decode(tokens[:max_tokens])\n",
    "\n",
    "def parsear_respuesta(respuesta: str):\n",
    "    relevancia = \"No Relevante\"\n",
    "    explicacion = \"No se pudo interpretar respuesta\"\n",
    "    for line in respuesta.split(\"\\n\"):\n",
    "        if line.lower().startswith(\"relevancia:\"):\n",
    "            relevancia = line.split(\":\", 1)[1].strip()\n",
    "        elif line.lower().startswith(\"explicaci√≥n:\"):\n",
    "            explicacion = line.split(\":\", 1)[1].strip()\n",
    "    return relevancia, explicacion\n",
    "\n",
    "for _, row in tqdm(df_nuevas.iterrows(), total=len(df_nuevas), desc=\"üîç Clasificando normativas\"):\n",
    "    texto_original = str(row[\"cuerpo\"])\n",
    "    metadatos = {\n",
    "        \"name\": row[\"name\"],\n",
    "        \"description\": row[\"description\"],\n",
    "        \"fuente\": row[\"fuente\"],\n",
    "        \"url\": row[\"url\"],\n",
    "        \"tipo_documento\": row.get(\"tipo_documento\", \"\")\n",
    "    }\n",
    "\n",
    "    relevancia = \"No Relevante\"\n",
    "    explicacion = \"Clasificaci√≥n fallback por error o exceso de tokens.\"\n",
    "\n",
    "    for factor in [1.0, 0.8, 0.6, 0.4, 0.2]:\n",
    "        try:\n",
    "             #texto_truncado = truncar_a_tokens(texto_original, int(SAFE_TOKENS * factor))\n",
    "\n",
    "            from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "            max_tokens = int(SAFE_TOKENS * factor)\n",
    "\n",
    "            # Si cabe completo, usar tal cual\n",
    "            if len(tokenizer.encode(texto_original)) <= max_tokens:\n",
    "                texto_truncado = texto_original\n",
    "            else:\n",
    "                splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "                chunks = splitter.split_text(texto_original)\n",
    "                \n",
    "                texto_truncado = \"\"\n",
    "                tokens_acumulados = 0\n",
    "                for chunk in chunks:\n",
    "                    chunk_tokens = tokenizer.encode(chunk)\n",
    "                    if tokens_acumulados + len(chunk_tokens) > max_tokens:\n",
    "                        break\n",
    "                    texto_truncado += chunk + \"\\n\"\n",
    "                    tokens_acumulados += len(chunk_tokens)\n",
    "\n",
    "            \n",
    "            docs_similares = vectorstore.similarity_search(texto_truncado, k=K)\n",
    "            contexto = \"\\n\\n\".join([doc.page_content for doc in docs_similares])\n",
    "            contexto_truncado = truncar_a_tokens(contexto, int(SAFE_TOKENS * (1.0 - factor)))\n",
    "\n",
    "            normativa = f\"Nombre: {metadatos['name']}\\nDescripci√≥n: {metadatos['description']}\\nFuente: {metadatos['fuente']}\\nTexto:\\n{texto_truncado}\"\n",
    "\n",
    "            respuesta = chain.run(context=contexto_truncado, normativa=normativa)\n",
    "            relevancia, explicacion = parsear_respuesta(respuesta)\n",
    "            break  # √âxito, salir del bucle\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fallo con factor {factor}: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    resultados.append({\n",
    "        \"name\": metadatos[\"name\"],\n",
    "        \"description\": metadatos[\"description\"],\n",
    "        \"fuente\": metadatos[\"fuente\"],\n",
    "        \"url\": metadatos[\"url\"],\n",
    "        \"tipo_documento\": metadatos[\"tipo_documento\"],\n",
    "        \"cuerpo\": texto_original,\n",
    "        \"relevancia\": relevancia,\n",
    "        \"explicacion\": explicacion,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a80f0cfc-010c-41d4-8d30-46b863cf5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clasificaci√≥n de normativas nuevas completada: data_ag1/normativas_nuevas_clasificadas.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- GUARDAR RESULTADOS ----------\n",
    "df_resultado = pd.DataFrame(resultados)\n",
    "df_resultado.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ Clasificaci√≥n de normativas nuevas completada: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01590be5-df44-457e-b3dd-ec91f4368aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>fuente</th>\n",
       "      <th>url</th>\n",
       "      <th>tipo_documento</th>\n",
       "      <th>cuerpo</th>\n",
       "      <th>relevancia</th>\n",
       "      <th>explicacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Circular N¬∞ 25 del 03 de Abril del 2025</td>\n",
       "      <td>Instruye sobre el reval√∫o de bienes ra√≠ces ubi...</td>\n",
       "      <td>Fuente: Subdirecci√≥n de Avaluaciones</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/circu...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>considerando la modificaci√≥n del referido art√≠...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circular N¬∞ 38 del 30 de Abril del 2025</td>\n",
       "      <td>Imparte instrucciones sobre la obtenci√≥n de ro...</td>\n",
       "      <td>Fuente: Subdirecci√≥n de Asistencia al Contribu...</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/circu...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>considerando estas modificaciones y la necesid...</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>La normativa es relevante porque modifica y co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Circular N¬∞ 42 del 15 de Mayo del 2025</td>\n",
       "      <td>Informa tabla de c√°lculos de reajustes y multa...</td>\n",
       "      <td>Fuente: Subdirecci√≥n de Fiscalizaci√≥n</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/circu...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>departamento emisor circular n¬∫42 ge00324967 s...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Resoluci√≥n Exenta SII N¬∞ 59 del 06 de Mayo del...</td>\n",
       "      <td>Establece procedimiento para efectuar denuncia...</td>\n",
       "      <td>Fuente: Subdirecci√≥n Jur√≠dica.</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resoluci√≥n</td>\n",
       "      <td>considerando 1 que el n1 de la letra a del art...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resoluci√≥n Exenta SII N¬∞ 79 del 26 de Junio de...</td>\n",
       "      <td>Instruye sobre la forma en que los √≥rganos de ...</td>\n",
       "      <td>Fuente: Subdirecci√≥n de Asistencia al Contribu...</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resoluci√≥n</td>\n",
       "      <td>considerando 1 que conforme lo dispuesto en lo...</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>La normativa modifica y complementa el Art√≠cul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Resoluci√≥n Exenta SII N¬∞ 81 del 30 de Junio de...</td>\n",
       "      <td>Modifica fecha de entrada en vigencia de la Re...</td>\n",
       "      <td>Fuente: Subdirecci√≥n de Fiscalizaci√≥n.</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resoluci√≥n</td>\n",
       "      <td>considerando 1¬∫ que la letra a del del art√≠cul...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Resoluci√≥n Exenta SII N¬∞ 83 del 03 de Julio de...</td>\n",
       "      <td>Autoriza como receptor electr√≥nico de document...</td>\n",
       "      <td>Fuente: Subdirecci√≥n de Asistencia al Contribu...</td>\n",
       "      <td>https://www.sii.cl/normativa_legislacion/resol...</td>\n",
       "      <td>Resoluci√≥n</td>\n",
       "      <td>considerando 1 que la resoluci√≥n exenta n 63 d...</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No cumple reglas de negocio. La normativa se c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0            Circular N¬∞ 25 del 03 de Abril del 2025   \n",
       "1            Circular N¬∞ 38 del 30 de Abril del 2025   \n",
       "2             Circular N¬∞ 42 del 15 de Mayo del 2025   \n",
       "3  Resoluci√≥n Exenta SII N¬∞ 59 del 06 de Mayo del...   \n",
       "4  Resoluci√≥n Exenta SII N¬∞ 79 del 26 de Junio de...   \n",
       "5  Resoluci√≥n Exenta SII N¬∞ 81 del 30 de Junio de...   \n",
       "6  Resoluci√≥n Exenta SII N¬∞ 83 del 03 de Julio de...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Instruye sobre el reval√∫o de bienes ra√≠ces ubi...   \n",
       "1  Imparte instrucciones sobre la obtenci√≥n de ro...   \n",
       "2  Informa tabla de c√°lculos de reajustes y multa...   \n",
       "3  Establece procedimiento para efectuar denuncia...   \n",
       "4  Instruye sobre la forma en que los √≥rganos de ...   \n",
       "5  Modifica fecha de entrada en vigencia de la Re...   \n",
       "6  Autoriza como receptor electr√≥nico de document...   \n",
       "\n",
       "                                              fuente  \\\n",
       "0               Fuente: Subdirecci√≥n de Avaluaciones   \n",
       "1  Fuente: Subdirecci√≥n de Asistencia al Contribu...   \n",
       "2              Fuente: Subdirecci√≥n de Fiscalizaci√≥n   \n",
       "3                     Fuente: Subdirecci√≥n Jur√≠dica.   \n",
       "4  Fuente: Subdirecci√≥n de Asistencia al Contribu...   \n",
       "5             Fuente: Subdirecci√≥n de Fiscalizaci√≥n.   \n",
       "6  Fuente: Subdirecci√≥n de Asistencia al Contribu...   \n",
       "\n",
       "                                                 url tipo_documento  \\\n",
       "0  https://www.sii.cl/normativa_legislacion/circu...       Circular   \n",
       "1  https://www.sii.cl/normativa_legislacion/circu...       Circular   \n",
       "2  https://www.sii.cl/normativa_legislacion/circu...       Circular   \n",
       "3  https://www.sii.cl/normativa_legislacion/resol...     Resoluci√≥n   \n",
       "4  https://www.sii.cl/normativa_legislacion/resol...     Resoluci√≥n   \n",
       "5  https://www.sii.cl/normativa_legislacion/resol...     Resoluci√≥n   \n",
       "6  https://www.sii.cl/normativa_legislacion/resol...     Resoluci√≥n   \n",
       "\n",
       "                                              cuerpo    relevancia  \\\n",
       "0  considerando la modificaci√≥n del referido art√≠...  No Relevante   \n",
       "1  considerando estas modificaciones y la necesid...     Relevante   \n",
       "2  departamento emisor circular n¬∫42 ge00324967 s...  No Relevante   \n",
       "3  considerando 1 que el n1 de la letra a del art...  No Relevante   \n",
       "4  considerando 1 que conforme lo dispuesto en lo...     Relevante   \n",
       "5  considerando 1¬∫ que la letra a del del art√≠cul...  No Relevante   \n",
       "6  considerando 1 que la resoluci√≥n exenta n 63 d...  No Relevante   \n",
       "\n",
       "                                         explicacion  \n",
       "0  No cumple reglas de negocio. La normativa se c...  \n",
       "1  La normativa es relevante porque modifica y co...  \n",
       "2  No cumple reglas de negocio. La normativa se c...  \n",
       "3  No cumple reglas de negocio. La normativa se c...  \n",
       "4  La normativa modifica y complementa el Art√≠cul...  \n",
       "5  No cumple reglas de negocio. La normativa se c...  \n",
       "6  No cumple reglas de negocio. La normativa se c...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuevas_clasificadas = pd.read_csv(\"data_ag1/normativas_nuevas_clasificadas.csv\")\n",
    "df_nuevas_clasificadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb0ec3-0004-42d9-9a83-b3dee0a880f6",
   "metadata": {},
   "source": [
    "## 4. Indexador de embeddings en Redis\n",
    "Por cada normativa nueva clasificada genera un √≠ndice en Redis para ser validada la clasificaci√≥n con intervenci√≥n humana. Una vez validada o corregida pasa a ser parte de la KB de normativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5ad24-a8c7-4229-9c80-e9eece0fb809",
   "metadata": {},
   "source": [
    "### 4.1 Prepara documentos\n",
    "Divide documentos en Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78a7d0a4-bff3-4075-9751-23832a7bab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Tokenizador para OpenAI embeddings\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
    "MAX_TOKENS = 30000\n",
    "SAFE_TOKENS = 25000  # margen de seguridad\n",
    "\n",
    "def contar_tokens(texto: str) -> int:\n",
    "    return len(tokenizer.encode(texto))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def resumen_chunks_por_documento(documentos):\n",
    "    conteo = Counter(doc.metadata.get(\"name\", \"Sin nombre\") for doc in documentos)\n",
    "    print(f\"Total de chunks generados: {len(documentos)}\")\n",
    "    print(\"Resumen por documento:\")\n",
    "    for nombre, cantidad in conteo.items():\n",
    "        print(f\"- {nombre}: {cantidad} chunk(s)\")\n",
    "\n",
    "def preparar_documentos(path_csv: str,\n",
    "                        n: int = None,\n",
    "                        anios: Union[int, List[int]] = None,\n",
    "                        chunk_size: int = None,\n",
    "                        chunk_overlap: int = None) -> List[Document]:\n",
    "    \n",
    "    df = pd.read_csv(path_csv).dropna(subset=[\"cuerpo\"])\n",
    "\n",
    "    # Filtro por a√±o\n",
    "    if anios:\n",
    "        if isinstance(anios, int):\n",
    "            anios = [anios]\n",
    "        df = df[df[\"name\"].astype(str).str.contains(\"|\".join([str(anio) for anio in anios]))]\n",
    "\n",
    "    if n:\n",
    "        df = df.head(n)\n",
    "\n",
    "    documentos = []\n",
    "    splitter = None\n",
    "\n",
    "    if chunk_size and chunk_overlap:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        metadatos = {\n",
    "            \"name\": row.get(\"name\", \"\"),\n",
    "            \"description\": row.get(\"description\", \"\"),\n",
    "            \"fuente\": row.get(\"fuente\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"tipo_documento\": row.get(\"tipo_documento\", \"\"),\n",
    "            \"relevancia\": row.get(\"relevancia\", \"\"),\n",
    "            \"explicacion\": row.get(\"explicacion\", \"\")\n",
    "        }\n",
    "\n",
    "        cuerpo = str(row[\"cuerpo\"]).strip()\n",
    "        total_tokens = contar_tokens(cuerpo)\n",
    "\n",
    "        if total_tokens <= SAFE_TOKENS:\n",
    "            documentos.append(Document(page_content=cuerpo, metadata=metadatos))\n",
    "        elif splitter:\n",
    "            partes = splitter.split_text(cuerpo)\n",
    "            for parte in partes:\n",
    "                if contar_tokens(parte) <= SAFE_TOKENS:\n",
    "                    documentos.append(Document(page_content=parte, metadata=metadatos))\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Chunk excede el m√°ximo seguro de tokens, se omiti√≥ parcialmente.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Documento omitido por superar l√≠mite de tokens y no hay splitter definido.\")\n",
    "\n",
    "    return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "089f5158-dd27-4ada-80c6-b68d891ea5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar muestra de documentos cargados\n",
    "documentos = preparar_documentos(\n",
    "    path_csv=\"data_ag1/normativas_nuevas_clasificadas.csv\",\n",
    "    n=None,\n",
    "    anios= None,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0ed1196-718d-479d-b085-7bbf7e2b386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks generados: 7\n",
      "Resumen por documento:\n",
      "- Circular N¬∞ 25 del 03 de Abril del 2025: 1 chunk(s)\n",
      "- Circular N¬∞ 38 del 30 de Abril del 2025: 1 chunk(s)\n",
      "- Circular N¬∞ 42 del 15 de Mayo del 2025: 1 chunk(s)\n",
      "- Resoluci√≥n Exenta SII N¬∞ 59 del 06 de Mayo del 2025: 1 chunk(s)\n",
      "- Resoluci√≥n Exenta SII N¬∞ 79 del 26 de Junio del 2025: 1 chunk(s)\n",
      "- Resoluci√≥n Exenta SII N¬∞ 81 del 30 de Junio del 2025: 1 chunk(s)\n",
      "- Resoluci√≥n Exenta SII N¬∞ 83 del 03 de Julio del 2025: 1 chunk(s)\n"
     ]
    }
   ],
   "source": [
    "resumen_chunks_por_documento(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1caeca-e79f-4dd8-890c-e97cb78baebc",
   "metadata": {},
   "source": [
    "### 4.2 Indexaci√≥n en Redis\n",
    "Registra normativas y sus embeddings en Redis. Cuando una normativa nueva es indexada quedar√° con el campo \"validado = 0\" , que significa que a√∫n no has sido validada por un humano. Mediante alguna GUI, el usuario podr√° indicar que ya est√° validada o corregida su relevancia y explicaci√≥n, pasando as√≠ a un valor \"validado=1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d74b42a-d2f3-4fde-8250-704e04ca361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexar_en_redis(documentos, redis_url, redis_index, gpt_key, eliminar_anteriores=False):\n",
    "    from langchain_community.vectorstores.redis import Redis\n",
    "    from langchain_community.embeddings import OpenAIEmbeddings\n",
    "    from langchain.schema import Document\n",
    "    import redis as redis_client\n",
    "    import traceback\n",
    "    import pandas as pd\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",  # Dimensi√≥n fija de 6144\n",
    "    openai_api_key=gpt_key\n",
    ")\n",
    "\n",
    "    if eliminar_anteriores:\n",
    "        try:\n",
    "            r = redis_client.from_url(redis_url)\n",
    "            r.ft(redis_index).dropindex(delete_documents=True)\n",
    "            print(f\"üßπ √çndice '{redis_index}' eliminado correctamente.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è No se pudo eliminar el √≠ndice '{redis_index}' (puede que no exista): {e}\")\n",
    "\n",
    "    index_schema = {\n",
    "        \"name\": \"TEXT\",\n",
    "        \"description\": \"TEXT\",\n",
    "        \"fuente\": \"TEXT\",\n",
    "        \"url\": \"TEXT\",\n",
    "        \"tipo_documento\": \"TEXT\",\n",
    "        \"relevancia\": \"TEXT\",\n",
    "        \"explicacion\": \"TEXT\",\n",
    "        \"validado\": \"NUMERIC\"\n",
    "    }\n",
    "\n",
    "    print(f\"üöÄ Iniciando indexaci√≥n de {len(documentos)} documentos (con control de tokens)...\")\n",
    "\n",
    "    errores = []\n",
    "    indexados = []\n",
    "    lote_actual = []\n",
    "    tokens_lote = 0\n",
    "\n",
    "    for i, doc in enumerate(documentos):\n",
    "        texto = doc.page_content\n",
    "        tokens_doc = contar_tokens(texto)\n",
    "\n",
    "        if tokens_doc > SAFE_TOKENS:\n",
    "            errores.append({\n",
    "                \"indice\": i + 1,\n",
    "                \"name\": doc.metadata.get('name', f'doc_{i}'),\n",
    "                \"tokens\": tokens_doc,\n",
    "                \"error\": \"Supera SAFE_TOKENS individualmente\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # üëá A√±adir campo \"validado\"\n",
    "        doc.metadata[\"validado\"] = 0\n",
    "        \n",
    "        if tokens_lote + tokens_doc > SAFE_TOKENS:\n",
    "            # Indexar lote actual\n",
    "            try:\n",
    "                Redis.from_documents(\n",
    "                    lote_actual,\n",
    "                    embedding=embeddings,\n",
    "                    redis_url=redis_url,\n",
    "                    index_name=redis_index,\n",
    "                    index_schema=index_schema\n",
    "                )\n",
    "                indexados.extend(lote_actual)\n",
    "                print(f\"‚úÖ Lote de {len(lote_actual)} documentos indexado.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error al indexar lote: {str(e)}\")\n",
    "                for j, d in enumerate(lote_actual):\n",
    "                    errores.append({\n",
    "                        \"indice\": i - len(lote_actual) + j + 1,\n",
    "                        \"name\": d.metadata.get('name', f'doc_{j}'),\n",
    "                        \"tokens\": contar_tokens(d.page_content),\n",
    "                        \"error\": f\"Error en lote: {str(e)}\"\n",
    "                    })\n",
    "            # Resetear lote\n",
    "            lote_actual = []\n",
    "            tokens_lote = 0\n",
    "\n",
    "        lote_actual.append(doc)\n",
    "        tokens_lote += tokens_doc\n",
    "\n",
    "    # Indexar √∫ltimo lote si qued√≥ algo pendiente\n",
    "    if lote_actual:\n",
    "        try:\n",
    "            Redis.from_documents(\n",
    "                lote_actual,\n",
    "                embedding=embeddings,\n",
    "                redis_url=redis_url,\n",
    "                index_name=redis_index,\n",
    "                index_schema=index_schema\n",
    "            )\n",
    "            indexados.extend(lote_actual)\n",
    "            print(f\"‚úÖ √öltimo lote de {len(lote_actual)} documentos indexado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en √∫ltimo lote: {str(e)}\")\n",
    "            for j, d in enumerate(lote_actual):\n",
    "                errores.append({\n",
    "                    \"indice\": len(indexados) + j + 1,\n",
    "                    \"name\": d.metadata.get('name', f'doc_{j}'),\n",
    "                    \"tokens\": contar_tokens(d.page_content),\n",
    "                    \"error\": f\"Error en lote final: {str(e)}\"\n",
    "                })\n",
    "\n",
    "    print(f\"üì¶ Total indexados: {len(indexados)} / {len(documentos)}\")\n",
    "\n",
    "    if errores:\n",
    "        df_err = pd.DataFrame(errores)\n",
    "        df_err.to_csv(\"errores_indexacion.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(\"üìÑ Errores registrados en 'errores_indexacion.csv'\")\n",
    "\n",
    "    return len(indexados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a625972b-b113-49f8-b56b-326f141131c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`index_schema` does not match generated metadata schema.\n",
      "If you meant to manually override the schema, please ignore this message.\n",
      "index_schema: {'name': 'TEXT', 'description': 'TEXT', 'fuente': 'TEXT', 'url': 'TEXT', 'tipo_documento': 'TEXT', 'relevancia': 'TEXT', 'explicacion': 'TEXT', 'validado': 'NUMERIC'}\n",
      "generated_schema: {'text': [{'name': 'name'}, {'name': 'description'}, {'name': 'fuente'}, {'name': 'url'}, {'name': 'tipo_documento'}, {'name': 'relevancia'}, {'name': 'explicacion'}], 'numeric': [{'name': 'validado'}], 'tag': []}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando indexaci√≥n de 7 documentos (con control de tokens)...\n",
      "‚úÖ √öltimo lote de 7 documentos indexado.\n",
      "üì¶ Total indexados: 7 / 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexar_en_redis(documentos, redis_url, redis_index, gpt_key, eliminar_anteriores=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
